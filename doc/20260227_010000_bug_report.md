# Bug Report - 2026-02-27 01:00 - 설치 및 운영 과정 버그 7건

## BUG-001: ssh_exec_sudo 따옴표 깨짐

### 원인
`ssh_exec_sudo` 함수가 `bash -c '$*'` 형태로 명령을 전달하여, sed 등 작은따옴표가 포함된 명령이 깨짐.

```
sed: -e expression #1, char 15: unterminated `s' command
```

### 해결 과정
`sudo bash -s` + heredoc 방식으로 변경. 또한 tart Ubuntu 이미지가 passwordless sudo를 지원하므로 password echo 제거.

```bash
# Before (broken)
ssh_exec "$ip" "echo '$password' | sudo -S bash -c '$*'"

# After (fixed)
sshpass -p "$password" ssh $SSH_OPTS "${user}@${ip}" sudo bash -s <<EOF
$*
EOF
```

### 결과
sed, iptables 등 특수문자 포함 명령이 정상 실행됨.

### 수정 파일
- `scripts/lib/ssh.sh` - ssh_exec_sudo 함수

---

## BUG-002: conntrack 패키지 미설치

### 원인
kubeadm init preflight 체크에서 conntrack이 필수인데 Ubuntu 기본 이미지에 미포함.

```
[ERROR FileExisting-conntrack]: conntrack not found in system path
```

### 해결 과정
`install_containerd` 함수의 apt-get install에 conntrack 추가.

```bash
apt-get install -y -qq containerd apt-transport-https ca-certificates curl gnupg conntrack
```

### 결과
kubeadm init preflight 체크 통과.

### 수정 파일
- `scripts/lib/k8s.sh` - install_containerd 함수

---

## BUG-003: VM 간 통신 불가 (Shared Network)

### 원인
Tart 기본 shared networking(NAT)에서 VM 간 직접 통신이 차단됨. 호스트→VM은 가능하지만 VM→VM은 불가.

```
$ ping -c 2 192.168.66.2  # worker1 → master
From 192.168.66.3 icmp_seq=1 Destination Host Unreachable
100% packet loss
```

kubeadm join이 API 서버에 접근 불가:
```
error execution phase preflight: couldn't validate the identity of the API Server:
failed to request the cluster-info ConfigMap: client rate limiter Wait returned an error:
rate: Wait(n=1) would exceed context deadline
```

### 해결 과정
1. `--net-bridged=en0` 시도 → shared IP가 무효화되어 SSH 접속 자체 불가
2. `--net-softnet-allow=0.0.0.0/0` 적용 → VM 간 통신 허용하는 소프트웨어 네트워킹

```bash
# Before
tart run "$vm_name" --no-graphics &

# After
tart run "$vm_name" --no-graphics --net-softnet-allow=0.0.0.0/0 &
```

IP 대역이 `192.168.66.x` → `192.168.65.x`로 변경됨.

### 결과
```
$ ping -c 2 192.168.65.35  # worker1(192.168.65.36) → master
64 bytes from 192.168.65.35: icmp_seq=1 ttl=64 time=0.524 ms
0% packet loss
```

### 수정 파일
- `scripts/lib/vm.sh` - vm_start 함수

---

## BUG-004: Cilium이 K8s API 서버에 접근 실패

### 원인
`--skip-phases=addon/kube-proxy`로 kube-proxy를 건너뛰었으므로, ClusterIP(10.96.0.1)로의 라우팅이 존재하지 않음. Cilium 자체가 이를 대체해야 하지만 부트스트랩 시점에 아직 동작 전.

```
level=error msg="Unable to contact k8s api-server"
ipAddr=https://10.96.0.1:443
error="dial tcp 10.96.0.1:443: i/o timeout"
```

### 해결 과정
Cilium Helm 설치 시 `k8sServiceHost`와 `k8sServicePort`를 마스터 노드 실제 IP로 지정.

```bash
helm upgrade --install cilium cilium/cilium \
  --set k8sServiceHost="$master_ip" \
  --set k8sServicePort=6443 \
  ...
```

### 결과
Cilium이 마스터 IP로 직접 API 통신하여 정상 부트스트랩. 3개 클러스터 모두 Cilium Running.

### 수정 파일
- `scripts/lib/k8s.sh` - install_cilium 함수

---

## BUG-005: wait_nodes_ready 함수 wc 파싱 에러

### 원인
`wc -l` 출력에 공백이 포함되어 `[[ "0\n99" -eq 0 ]]` 비교가 실패.

```
/Users/ywlee/tart-infra/scripts/lib/k8s.sh: line 129:
[[: 0
99: syntax error in expression (error token is "99")
```

### 해결 과정
```bash
# Before
not_ready=$(kubectl_cmd ... | grep -v "Ready" | wc -l || echo "99")

# After
not_ready=$(kubectl_cmd ... | grep -cv " Ready " || true)
```

### 결과
노드 Ready 상태 체크가 정상 동작.

### 수정 파일
- `scripts/lib/k8s.sh` - wait_nodes_ready 함수

---

## BUG-006: Jenkins PVC Pending + Values 키 변경

### 원인
kubeadm 기본 설치에는 StorageClass가 없어 PVC가 바인딩되지 않음.
또한 Jenkins Helm 차트에서 `controller.adminPassword` → `controller.admin.password`로 키가 변경됨.

```
Warning  FailedScheduling  pod has unbound immediate PersistentVolumeClaims
```
```
Error: `controller.adminPassword` no longer exists. It has been renamed to `controller.admin.password`
```

### 해결 과정
1. local-path-provisioner 설치 및 default StorageClass 지정
2. jenkins-values.yaml 키 수정

```yaml
# Before
controller:
  adminPassword: admin

# After
controller:
  admin:
    password: admin
```

### 결과
Jenkins PVC가 local-path로 바인딩되어 정상 Running.

### 수정 파일
- `scripts/install/08-install-cicd.sh` - local-path-provisioner 설치 추가
- `manifests/jenkins-values.yaml` - admin.password 키 변경

---

## BUG-007: prod-master kubeadm init CPU 부족 오류

- **발견 시각**: 2026-02-27 14:40 KST
- **심각도**: High
- **카테고리**: Configuration

### 원인
prod-master VM에 CPU를 1개만 할당했으나, kubeadm init의 preflight 체크에서 최소 2 CPU를 요구하여 초기화 실패.

```
[ERROR NumCPU]: the number of available CPUs 1 is less than the required 2
```

### 해결 과정
1. prod-master VM의 CPU 할당을 1에서 2로 변경
2. `config/clusters.json`의 prod-master cpu 값을 2로 수정
3. `terraform/variables.tf`의 prod-master cpu 값을 2로 수정
4. VM 재시작 후 kubeadm init 성공

```json
// Before
{ "name": "prod-master", "role": "master", "cpu": 1, "memory": 3072, "disk": 20 }

// After
{ "name": "prod-master", "role": "master", "cpu": 2, "memory": 3072, "disk": 20 }
```

### 결과
prod-master에서 kubeadm init이 정상 완료되어 prod 클러스터 구성 성공.

### 수정 파일
- `config/clusters.json` - prod-master cpu: 1 → 2
- `terraform/variables.tf` - prod-master cpu: 1 → 2
